{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce55d524-7fc3-4a0a-b5db-cb359a1b28c4",
   "metadata": {},
   "source": [
    "The file downloads and save collected pdf and other txt files from orbit.\n",
    "\n",
    "##### Will need orbit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568035e7-2732-41b1-8a0a-797b309819d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d258b2f-85c1-4852-9295-4410455b5e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Orbit-provided keys ---\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=\"\",\n",
    "    aws_secret_access_key=\"\"\n",
    ")\n",
    "\n",
    "bucket_name = \"orbit-data-provider\"\n",
    "prefix = \"clients/eagle-alpha/\"\n",
    "base_dir = \"data/reports\"\n",
    "os.makedirs(base_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00972309-d730-4d2e-a28c-8346e9df6268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dictionary of existing files once\n",
    "def build_existing_files(base_dir=\"data/reports\"):\n",
    "    existing = set()\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for f in files:\n",
    "            existing.add(os.path.join(root, f))\n",
    "    return existing\n",
    "\n",
    "existing_files = build_existing_files(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1e08e3-a0b1-4b41-af0c-e34e059b8774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download one file\n",
    "def download_file(presigned_url, file_path):\n",
    "    if file_path in existing_files:   # O(1) set lookup\n",
    "        print(f\"⏩ Skipping {file_path}, already exists\")\n",
    "        return file_path\n",
    "\n",
    "    try:\n",
    "        r = requests.get(presigned_url, stream=True, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        existing_files.add(file_path)   # update the set\n",
    "        print(f\"Downloaded {file_path}\")\n",
    "        return file_path\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a8863d-1000-4c5f-ae52-d657abce1f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save report given JSON index (parallel for files)\n",
    "def save_report(index_data, base_dir=\"data/reports\", max_workers=4):\n",
    "    company_name = index_data[\"company_info\"][0][\"company_name\"]\n",
    "    safe_name = company_name.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    company_folder = os.path.join(base_dir, safe_name)\n",
    "    os.makedirs(company_folder, exist_ok=True)\n",
    "\n",
    "    report_id = index_data[\"report_id\"]\n",
    "    futures = []\n",
    "    results = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for rf in index_data.get(\"report_files\", []):\n",
    "            for s3_key in [\"s3_path_file\", \"s3_path_pages\", \"s3_path_blocks\"]:\n",
    "                if s3_key in rf and rf[s3_key]:\n",
    "                    s3_path = rf[s3_key]\n",
    "                    bucket_name, key_path = s3_path.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "\n",
    "                    presigned_url = s3_client.generate_presigned_url(\n",
    "                        \"get_object\",\n",
    "                        Params={\"Bucket\": bucket_name, \"Key\": key_path},\n",
    "                        ExpiresIn=3600,\n",
    "                    )\n",
    "\n",
    "                    if s3_key.endswith(\"file\"):\n",
    "                        fname = f\"{report_id}_file.pdf\"\n",
    "                    elif s3_key.endswith(\"pages\"):\n",
    "                        fname = f\"{report_id}_pages.txt\"\n",
    "                    else:\n",
    "                        fname = f\"{report_id}_blocks.txt\"\n",
    "\n",
    "                    file_path = os.path.join(company_folder, fname)\n",
    "                    futures.append(executor.submit(download_file, presigned_url, file_path))\n",
    "\n",
    "        for f in as_completed(futures):\n",
    "            results.append(f.result())\n",
    "\n",
    "    return {\n",
    "        \"report_id\": report_id,\n",
    "        \"company_name\": company_name,\n",
    "        \"isin\": index_data[\"company_info\"][0].get(\"isin\", [None])[0],\n",
    "        \"ticker\": index_data[\"company_info\"][0].get(\"ticker\", [None])[0],\n",
    "        \"reported_at\": index_data.get(\"reported_at\"),\n",
    "        \"pdf_path\": os.path.join(company_folder, f\"{report_id}_file.pdf\"),\n",
    "        \"pages_path\": os.path.join(company_folder, f\"{report_id}_pages.txt\"),\n",
    "        \"blocks_path\": os.path.join(company_folder, f\"{report_id}_blocks.txt\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d36773-de24-4038-aa3f-521d80c02634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over JSON indexes in S3 (serial per report, parallel per file)\n",
    "def collect_all_reports(s3_client, bucket, prefix, base_dir=\"data/reports\"):\n",
    "    records = []\n",
    "    continuation_token = None\n",
    "\n",
    "    while True:\n",
    "        if continuation_token:\n",
    "            response = s3_client.list_objects_v2(\n",
    "                Bucket=bucket, Prefix=prefix, ContinuationToken=continuation_token\n",
    "            )\n",
    "        else:\n",
    "            response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "        for obj in response.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "            if not key.endswith(\".json\"):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                resp = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "                index_data = json.loads(resp[\"Body\"].read().decode(\"utf-8\"))\n",
    "                meta = save_report(index_data, base_dir, max_workers=4)\n",
    "                if meta:\n",
    "                    records.append(meta)\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed on {key}: {e}\")\n",
    "\n",
    "        if response.get(\"IsTruncated\"):\n",
    "            continuation_token = response[\"NextContinuationToken\"]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d99e31f-4318-4d24-8d67-82455c6a091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run collection\n",
    "meta_df = collect_all_reports(s3_client, bucket_name, prefix, base_dir)\n",
    "\n",
    "if not meta_df.empty:\n",
    "    meta_df[\"reported_at\"] = pd.to_datetime(meta_df[\"reported_at\"])\n",
    "    print(\"✅ Downloaded files and built meta_df:\", meta_df.shape)\n",
    "    meta_df.to_csv(\"meta_reports.csv\", index=False)\n",
    "else:\n",
    "    print(\"No new reports to process.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
